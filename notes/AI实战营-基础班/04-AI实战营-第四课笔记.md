# OpenMMLab AI实战营 第四课笔记

---
![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230202115351337-523975146.jpg)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230203174116047-204135633.png)

[TOC]

# 目标检测与MMDetection

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206210407131-527475089.png)


# 1.什么是目标检测

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206210516138-1877323993.png)

## 1.1 目标检测的应用

### 1.1.1 目标检测 in 人脸识别

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206210527670-1682427341.png)

> 人脸检测是目标检测的一种特殊情形。目标检测（通用物体检测）针对的是多类别，人脸检测是二分类，只检测人脸这个类别。
>
> 通用物体检测算法都可以直接拿来做人脸检测，只需要改一下输出类别就可以。但是如果直接拿来用，会出现一些问题：
>
> 通用物体检测考虑的是更广泛通用的物体，这些类别具有宽泛的意义。这些物体具有多种特点，场景复杂多变，物体形状、背景、大小、尺寸等都比人脸这种单一的类别更复杂。人脸检测虽然类别单一，但也不是那种简单的检测任务，人脸的角度、背景光照、尺度、类人脸的干扰物体、极小人脸等都是人脸检测里面的难题。而且通用目标检测模型对于人脸检测来说存在冗余，并且缺乏对人脸数据针对性的设计（如anchor设计，尤其是对于尺度范围很大的人脸检测场景来说基本上就很难训练出好的结果了）。因此在通用物体上表现好的模型在人脸检测上不一定表现的好。
>
> 人脸检测的问题可以针对性的解决，如anchor的设置、背景的处理、抑制误检、漏检等。要想直接将SOTA通用物体检测算法直接套在人脸检测上，不一定能达到最好的效果，需要具体情况具体分析。
>
> 基于目标检测在人脸识别上的应用，可以扩展到以下应用：
>
> - 身份识别
> - 属性分析

### 1.1.2 目标检测 in 智慧城市

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206210813719-1749812769.png)

> 目标检测是智慧城市的眼睛和窗口。
> 简单来说，主要包括以下场景中的应用：
>
> - **垃圾检测**：保持城市干净整洁
> - **非法占道检测**
> - **违章停车检测**：确保行人安全
> - **危险物体检测**：确保道路安全
> - **自助服务、智能办公**等
> - **烟雾和火灾侦测**：确保环境安全等
> - **标准着装监测、违规吸烟监测等**
> - **危险行为监测**：保障医院安全


### 1.1.3 目标检测 in 自动驾驶

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206210828953-1083398390.png)

> 目标检测在自动驾驶中的应用，主要体现在环境感知和路径规划与控制中。
>
> ![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230219174413493-458440887.png)
> 自动驾驶在道路交通上的感知功能主要包括以下三个方面：
>
> - **动态目标监测**(车辆、行人和非机动车)
> - **静态物体识别**(交通标志和红绿灯)
> - **可行驶区域的分割**(道路区域和车道线) 
>
> 目标检测难点：
>
> - 遮挡情况较多，朝向角准确性问题；
> - 行人车辆类型种类较多，容易误检；
> - 多目标追踪，ID切换问题；
>
> 对于视觉目标检测，在恶劣天气环境下，感知性能会有一定的下降；在夜晚灯光昏暗时，容易出现漏检的问题。如果结合激光雷达的结果进行融合，对于目标的召回率会大幅提高。


### 1.1.4 目标检测 in 下游视觉任务

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206210847589-1996266926.png)

> 类似地，目标检测在其他下游视觉任务中的应用也有很多，例如：
>
> - OCR识别中，使用目标检测来检测文字出现的区域，从而识别区域中的文字
> - 在人体姿态估计中，使用目标检测来定位

## 1.2 目标检测 vs 图像分类

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206211056348-2007180370.png)

> 可以来对比下图像分类与目标检测的异同
> 不同之处在于，图像分类通常只有一个物体，且主体通常位于图像中央，并占据主要面积；而目标检测中，物体数量不固定，物体位置不固定，物体大小也不固定。
> 相同之处在于，二者都需要算法去“理解”图像的内容，并通过深度神经网络来实现。

### 1.2.1 滑动窗口Sliding Window

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206211209810-1868917746.png)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206211226957-105684303.png)

> **滑动窗口的过程**：
>
> - 1.设定一个固定大小的窗口
> - 2.遍历图像所有位置，所到之处用分类模型(假设已经训练好)识别窗口中的内容。
> - 3.为了检测不同大小、不同形状的物体，可以使用不同大小、长宽比的窗口扫描图片。
>
> 首先固定一个于滑动窗口区域，然后将滑动窗口在图像上按照指定步长进行滑动，对于每一的滑动得到区域进行预测，判断该区域中存在目标的概率。
>
> 调整滑动窗口的大小、滑动步长，继续以同样的方式滑动，预测。
>

### 1.2.2 滑窗的效率问题

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206211244721-1889085197.png)

> 滑动窗口目标检测算法也有很明显的缺点，就是计算成本，因为你在图片中剪切出太小方块，卷积网络要一个个地处理。如果你选用的步幅很大，显然会减少输入卷积网络的窗口个数，但是粗糙间隔尺寸可能会影响性能。反之，如果采用小粒度或小步幅，传递给卷积网络的小窗口会特别多，这意味着超高的计算成本。
> 另外，滑动窗本身窗口大小大部分情况下并不完全贴合目标尺度；一张图滑动窗预测后，会出输多个结果，需对输出结果做NMS（非极大值抑制)，再作为最终输出结果。
>
> 改进的思路有两种：
>
> - 使用启发式算法替换暴力遍历
> - 减少冗余计算，使用卷积网络实现密集预测
>   (这是目前普遍采用的方式。)

#### 1.2.2.1 改进思路1：区域提议

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206211524474-639407115.png)

>**改进思路1是区域提议**
>
>- 基于图像颜色或底层特征，找出可能含有物体的区域，再送给神经网络识别。
>- 相比于普通滑窗，减少了框体的个数，从而保证召回率。

#### 1.2.2.2 改进思路2：分析滑动窗口中的重复计算

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206211546050-1687404268.png)

> 改进思路2是分析滑窗中的重复计算
> 通过分析，我们可以知道，在卷积层中重叠部分用相同的卷积核计算了两次。
> 那么自然地，可以通过消除滑窗中的重复计算来进行改进。

#### 1.2.2.3 消除滑窗中的重复计算

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206211602668-1123869895.png)

> 在原图滑窗中，卷积计算重复计算了重叠区域。
> 消除滑窗中重复计算的改进思路是。用卷积一次性计算所有特征，再取出对应位置的特征完成分类。
> 也就是基于特征图滑窗，这样一来，原来的重叠区域只计算了一次卷积特征，则计算复杂度与窗的个数无关。

#### 1.2.2.4 在特征图上进行密集预测

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206211617691-610011844.png)
 >另一方面，在特征图上进行密集预测，不同特征的感受野自然形成一系列等距离分布的窗。
 >密集预测实际上是一种隐式的滑窗方法，计算效率远高于滑窗。

## 1.3 目标检测的基本范式

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206211636506-1026833601.png)

> 这里简单概述一下目标检测的基本范式。
> 目标检测主要可以归纳为两种方法：
>
> - **单阶段方法**
>   以某种方式产生窗体，再基于窗口内的特征进行预测。
> - **双阶段方法**
>   在特征图上基于单点特征实现密集预测。
>
> 对于目标检测神经网络模型，包括**主干网络(Backbone)**和**检测头(Head)**等结构。

## 1.4 目标检测技术的演进

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206211647857-1794416008.png)

> 目标检测技术演进：从R-CNN、Fast R-CNN、Faster R-CNN到YOLO、SSD。
>
> 目前学术和工业界出现的目标检测算法分成3类：
>
> - 1.**传统的目标检测算法**：
>   Cascade + HOG/DPM + Haar/SVM以及上述方法的诸多改进、优化；
> - 2.**候选区域/框 + 深度学习分类**：通过提取候选区域，并对相应区域进行以深度学习方法为主的分类的方案，如：
>   - R-CNN（Selective Search + CNN + SVM）SPP-net（ROI Pooling）
>   - Fast R-CNN（Selective Search + CNN + ROI）
>   - Faster R-CNN（RPN + CNN + ROI）R-FCN等系列方法；
> - 3.**基于深度学习的回归方法**：
>   - YOLO/SSD/DenseBox 等方法；
>   - 以及最近出现的结合RNN算法的RRC detection；
>   - 结合DPM的Deformable CNN等
> - **多尺度技术**
>   FPN
> - **级联方法**
>   Cascade R-CNN、HTC
> - 可参考的材料：
>   - 深度卷积神经网络在计算机视觉中的应用研究综述-卢宏涛
>   - 基于深度学习的计算机视觉研究新进展-卢宏涛

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206211707645-229704527.png)

> 从目标检测技术的演进过程来看，我们可以从深度学习网络的推理精度、推理速度以及模型体积来讨论。
>
> - **推理精度**(mAP on COCO)
>   **Faster R-CNN**(21-34%)——>**Cascade R-CNN**(42.8%)——>**HTC**(47.1%)——>**Deformable DETR**(52.3%)——>**SwinTrans+HTC**(57.7%)
> - **推理速度**
>   **R-CNN**(40s)<——**Faster R-CNN**(100-200ms)<——**RetinaNet**(73-198ms)<——**YOLO**(22ms)<——**SSD**(21ms)<——**YOLOX-s**(10ms)<——**YOLOv5**(6ms)
> - **模型体积**
>   **YOLOv3**(65MB)——>**Faster R-CNN**(42MB)——>**RetinaNet**(34MB)——>**YOLOv5n**——>**YOLOX-Nano**(0.9MB)

# 2.基础知识

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206211835850-1269939739.png)

## 2.1 框，边界框(Bounding Box)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206211858505-2011491708.png)

> **框**泛指图像上的**矩形框**，边界横平竖直
> 描述一个框，需要4个像素值：
>
> - 方式1：左上,右下边界坐标$(l. t, r, b)$.
> - 方式2：中心坐标和框的长宽$(x,y,w,h)$.
>
> **边界框**，通常指紧密包围感兴趣物体的框
> 检测任务，要求为图中出现的每个物体预测一个边界框。

## 2.2 框相关的概念

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206211914762-1639732460.png)

> **框相关的概念**
>
> - **1.区域**(Region)：框的同义词。
> - **2.区域提议**(Region Proposal,Proposal)
>   指算法预测的可能包含物体的框，某种识别能力不强的算法的初步预测结果。
> - **3.感兴趣区域**(Region of interset,ROI)
>   当我们谈论需要进一步检测这一框中是否有物体时，通常称为**感兴趣区域**。
> - **4.锚框**(Anchor Box,Anchor)
>   图中预设的一系列基准框，类似滑窗，一些检测算法会基于锚框预测边界框。

## 2.3 交并比(Intersection Over Union)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206211935161-1954662072.png)

> **交并比(IoU)**，定义为两矩阵交集面积与并集面积之比，是矩阵框重合程度的衡量指标。
> 在目标检测任务中，通常会使用**交并比（Intersection of Union，IoU）**作为衡量指标，来衡量两个矩形框之间的关系。 
> 例如在基于锚框的目标检测算法中，我们知道当锚框中包含物体时，我们需要预测物体类别并微调锚框的坐标，从而获得最终的预测框。

## 2.4 置信度

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206212138998-1083588074.png)

> **置信度(Confidence Score)**：模型认可自身预测结果的程度，通常需要为每个框预测一个置信度。
>
> - 大部分算法取分类模型预测物体属于特定类别的概率。
> - 部分算法让模型独立于分类单独预测一个置信度。
> - 我们倾向于置信度高的预测结果。


## 2.5 非极大值抑制

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206212147441-1878859583.png)

> **非极大值预测 nms**
> 滑窗类算法，通常会在物体周围给出多个相近的检测框。这些框实际指向同一物体，只需要保留其中置信度最高的。
>
> - 通过非极大值预测(NMS)
>
> - 大致流程：
>
>   - 1.选取这类box中scores最大的那一个，记为current_box，并保留它(为什么保留它，因为它预测出当前位置有物体的概率最大啊，对于我们来说当前confidence越大说明当前box中包含物体的可能行就越大)
>
>   - 2.计算current_box与其余的box的IOU
>
>   - 3.如果其IOU大于我们设定的阈值，那么就舍弃这些boxes（由于可能这两个box表示同一目标，因此这两个box的IOU就比较大，会超过我们设定的阈值，所以就保留分数高的那一个）
>
>   - 4.从最后剩余的boxes中，再找出最大scores的那一个(之前那个大的已经保存到输出的数组中，这个是从剩下的里面再挑一个最大的)，如此循环往复。

## 2.6 边界框回归

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206212323316-203820169.png)

> **问题**
> 滑窗(或其他方式产生的基准框)与物体精准边界通常有偏差。
> **处理方法**
> 让模型在预测物体类别同时预测边界框相对于滑窗的偏移量。
> 在多任务学习中，卷积网络处理特征的方式不同。
>
> - $C+1$维分类概率——>**分类损失**
> - 4维偏移量——>**回归损失**


## 2.7 边界框编码

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206212331739-519656262.png)

> **边界框编码Bbox Coding**
> 边界框的绝对偏移量在数值上通常较大，不利于神经网络训练，通常需要对偏移量进行编码，作为回归模型的预测目标。
>
> - 例R-CNN系列算法的编码方案：对于基准框$P(p_x, p_y, p_w)$和真值框$G=(g_x, g_y, g_h, g_w)$。
>   编码值$T = (t_x, t_y, t_h, t_w) = \left(\frac{g_x - p_x}{p_w}. \frac{g_y-p_y}{p_h},log(\frac{g_w}{p_w}),log(\frac{g_h}{p_h}) \right)$.
> - 推理时，已知基准框$P$和回归模型预测的偏移量编码$T$，解码得到预测框。


# 3.两阶段目标检测算法


![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206212350244-364431499.png)

## 3.1 两阶段算法概述

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206212437485-1531240486.png)

> R-CNN是第一个用深度学习解决目标检测问题的算法，也是两阶段法的奠基者，因包含区域提议和区域识别两个阶段得名。
>
> 经历一系列发展到Faster R-CNN和Mask R-CNN逐渐成熟。
>
> - **R-CNN流程**：
>   区域提名（SS） --> CNN提取特征 --> SVM分类 --> 边框回归
>
> - **SPPNet流程：**
>   CNN提取特征 --> 区域提名（SS） --> SVM分类 --> 边框回归
>
> - **Fast R-CNN流程：**
>   CNN提取特征 --> 区域提名（SS） --> 分类（softmax）+边框回归
>
> - **Faster R-CNN流程：**
>
>   CNN提取特征 --> 区域提名（RPN） --> 分类（softmax）+边框回归
>
> 结合比较先进的主干网络和多尺度技术，可以达到比较优越的检测精度，使用广泛。
> 近几年(2020-)随着单阶段算法精度和速度的提高逐渐被取代。

## 3.2 Region-based CNN(2013)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206212451312-988148960.png)

> R-CNN的两阶段分别为以下：
>
> - **Stage1 产生提议框**
>   使用传统视觉算法，推测可能包含物体的框(约2000个)。
>   - 不漏：真正包含物体的框，通常会被选中；
>   - 不准：大部分提议框并不包含物体；
> - **Stage2 识别提议框**
>   将提议框内的图像缩放至固定大小(原始论文227x227)，送入卷积网络进一步识别，得到准确结果。
> - **两阶段的流程**为：
>   输入图像——>区域提议——>裁剪缩放——>分类——>汇总所有框体再NMS产生最终结果。

### 3.2.1 R-CNN的训练

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206212507421-1485682306.png)

> **R-CNN的训练流程**：①——>②——>③.
>
> - ①产生提议框——裁剪缩放
> - ②生成用于训练分类网络的类别标签
>   - ——> 与某个真值IoU大于0.5 
>     - ——> 类别：真值框的类别=人
>       边界框偏移量=编码(真值框-提议框)
>   - ——> 与图中所有真值框IoU都小于0.5 
>     - ——> 类别：背景区域
>       边界框偏移量=0：不计入回归损失
> - ③收集所有图像块及对应的类别标签，训练分类模型
> - (固定规则的算法，则不需要训练)

### 3.2.2 R-CNN相比于传统方法的提升

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206212523756-2111881941.png)

> R-CNN相比于传统方法的提升
>
> 传统的目标检测框架，主要包括三个步骤：
>
> - ① 利用不同尺寸的滑动窗口框住图中的某一部分作为候选区域；
> - ② 提取候选区域相关的视觉特征。比如人脸检测常用的 Harr 特征；行人检测和普通目标检测常用的 HOG 特征等；
> - ③ 利用分类器进行识别，比如常用的 SVM 模型。
>
> **R-CNN核心思想：** 对每张图片选取多个区域，然后每个区域作为一个样本进入一个卷积神经网络来抽取特征。
>
> - **1.R-CNN网络结构**
>
> R-CNN算法是较早提出的两阶段目标检测算法，它先找出 Region Proposal，再进行分类和回归。
>
> - 所谓 Region Proposal 就是图中目标可能出现的位置。
> - 因为传统方法需要枚举的区域太多了，所以通过利用图像中的纹理、边缘、颜色等信息，可以保证在选取较少窗口（几千甚至几百）的情况下保持较高的响应比。所以，问题就转变成找出可能含有物体的候选框，这些框之间是可以互相重叠互相包含的，这样我们就可以避免暴力枚举的所有框了。


### 3.2.3 R-CNN的问题

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206212534070-1745540352.png)

> - **2.R-CNN应用流程**
>
> 对于每张输入的图像，R-CNN目标检测主要包括下述步骤：
>
> - ① 利用选择性搜索 Selective Search 算法在图像中从下到上提取 2000个左右的可能包含物体的候选区域 Region Proposal
> - ② 因为获取到的候选区域大小各不相同，所以需要将每个 Region Proposal 缩放(warp)成统一的$227 \times 227$的大小并输入到 CNN，将CNN的 fc7 层的输出作为特征
> - ③ 将每个 Region Proposal 提取到的CNN特征输入到SVM进行分类
> - ④ 使用这些区域特征来训练线性回归器对区域位置进行调整


## 3.3 Fast R-CNN(2014)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206212547732-1774016402.png)

> **Fast R-CNN**
> Fast R-CNN，主要是基于R-CNN的基础上做了改进，减少了重复计算。
> 可以从两阶段方法来考虑优化的方式
>
> - **Stage1 产生提议框**
>   仍然依赖传统CV方法。
> - **Stage2 识别提议框**
>   - 1.卷积层应用于全图，一次性计算所有位置的图像特征。
>   - 2.剪裁提议框对应的特征图送入全连接层计算分类。
> - 问题：提议框大小不同，需要处理成**固定尺寸**才能送入**全连接层**。

### 3.3.1 Rol Pooling

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206212601223-1248140045.png)

> **Rol Pooling**
> **目标**：将不同尺寸的提议框处理成相同尺寸，使之可以送入后续的全连接层计算分类和回归。
> **过程**：全图特征图与提议框——>切分提议区域——>区域内池化——>池化结果。
> 算法：
>
> - 1.将提议框切分成固定数目的格子；
> - 2.如果格子边界不在整数坐标，则膨胀至整数坐标；
> - 3.在每个格子内部池化，得到固定尺寸的输出特征图。
>
> 值得一提的是，输出的feature maps的大小不取决于ROI和卷积feature maps大小。**ROI Pooling**最大的好处就在于极大地提高了处理速度。


### 3.3.2 Rol Align

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206212614475-804638504.png)

> **ROI Align**是在**Mask-RCNN**这篇论文里提出的一种区域特征聚集方式, 很好地解决了ROI Pooling操作中两次量化造成的区域不匹配(mis-alignment)的问题。实验显示，在检测测任务中将 ROI Pooling 替换为 ROI Align 可以提升检测模型的准确性。
>
> - 将提议区域切成固定数目的格子，例如$7 \times 7$;
> - 在每个格子中，均匀选取若干采样点，如$2 \times 2 = 4$.
> - 通过插值方法得到每个采样点处的精确特征。
> - 所有采样点做Pooling得到输出结果。
>
> Rol Align比Rol Pooling在位置上更精细。

## 3.4 Fast R-CNN

### 3.4.1 Fast R-CNN的训练

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206212800849-1065592564.png)


### 3.4.2 Fast R-CNN的速度提升

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206212814441-355448479.png)

### 3.4.3 Fast R-CNN的精度提升

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206212921170-1914981141.png)


### 3.4.4 Fast R-CNN的速度瓶颈

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206212933475-1580228511.png)


### 3.4.5 降低区域提议的计算成本

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206212947012-1411831840.png)


### 3.4.6 朴素方法的局限

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213000594-307524314.png)

### 3.4.7 锚框Anchor

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213012778-186861234.png)


## 3.5 Faster R-CNN(2015)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213023674-721404125.png)


### 3.5.1 Faster R-CNN的训练

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213032252-2005290878.png)


## 3.6 两阶段方法的发展与演进(2013~2017)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213043072-1594543778.png)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213109494-1077806714.png)


# 4.多尺度检测技术

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213135381-91667023.png)

## 4.1 多尺度检测必要性

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213242432-792144139.png)


## 4.2 图像金字塔Image Pyramid

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213256512-1113158574.png)


## 4.3 层次化特征

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213305848-1846800560.png)


## 4.4 特征金字塔网络Feature Pyramid Network(2016 )

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213333856-1771236802.png)


## 4.5 在Faster R-CNN模型中使用FPN

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213342507-1593747505.png)


# 5.单阶段目标检测算法

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213353221-1907162432.png)


## 5.1 回顾两阶段算法

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213418804-488038408.png)


## 5.2 单阶段算法

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213430392-544765650.png)


## 5.3 单阶段检测算法概述

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213447868-749508566.png)

## 5.4 YOLO：You Only Look Once(2015)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213503092-1786874179.png)

### 5.4.1 YOLO的分类和回归目标

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213518389-1990025947.png)


### 5.4.2 YOLO的损失函数

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213530435-1461775878.png)


### 5.4.3 YOLO的优点和缺点

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213551177-1418812474.png)

## 5.5 SSD：Single Shot MultiBox Detector(2016)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213608787-70940309.png)

### 5.5.1 SSD的损失函数

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213625634-425704945.png)

### 5.5.2 正负样本不均衡问题

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213642375-2002325421.png)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213659613-1387074125.png)


### 5.5.3 解决样本不均衡问题

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213711837-677711603.png)


### 5.5.4 困难负样本Hard Negative

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213759640-1817489126.png)

### 5.5.5 不同负样本对损失函数的贡献

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213814886-692296099.png)


### 5.5.6 降低简单负样本的损失

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213826424-1781553116.png)


### 5.5.6 Focal Loss

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213840442-882656462.png)

## 5.6 RetinaNet(2017)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213854385-1689301347.png)

### 5.6.1 RetinaNet的性能

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213909581-1265990946.png)


## 5.7 YOLOv3(2018)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213921401-1815904438.png)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213929241-1009053484.png)





# 6.无锚框目标检测算法

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213939631-1623926172.png)

## 6.1 锚框 vs 无锚框

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206213954377-293558567.png)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214116471-1336320247.png)


## 6.2 FCOS,Fully Convolutinal One-Stage(2019)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214129278-1014344688.png)

### 6.2.1 FCOS的多尺度匹配

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214141902-2086153085.png)

### 6.2.2 FCOS的预测目标

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214155653-1398052445.png)


### 6.2.3 中心度Center-ness

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214203659-937647413.png)


### 6.2.4 FCOS的损失函数

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214213954-891480305.png)


## 6.3 CenterNet(2019)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214226408-1036357754.png)


### 6.3.1 CenterNet的主要流程

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214237419-247415458.png)


# 7.Detection Transformers

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214318981-1591334143.png)

## 7.1 DETR(2020)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214331847-1024317032.png)


## 7.2 Deformable DETR(2021)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214339274-533680411.png)



# 8.目标检测模型的评估方法Evaluaion

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214350397-512336548.png)


## 8.1 检测结果的正确/错误类型

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214455382-371669508.png)

## 8.2 准确率Rrecision与召回率Recall

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214526352-141773567.png)


## 8.3 准确率与召回率的平衡

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214538558-500025700.png)


## 8.4 PR曲线与AP值

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214605994-188522972.png)


## 8.5 完整数据集上的例子

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214630992-784474975.png)


## 8.6 PR曲线的起伏

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214640516-1374170178.png)


## 8.7 Mean AP

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214648257-1773129659.png)


## 8.8 总结

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214657522-968939591.png)




# 9.MMDetection

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214711979-254153639.png)


## 9.1 目标检测工具包MMDetection

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214722703-1761140122.png)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214743989-96723981.png)

## 9.2 广泛应用

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214752549-71698701.png)

## 9.3 MMDetection可以做什么

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214826904-2132595040.png)


## 9.4 MMDetection环境搭建

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214835408-1329556292.png)


## 9.5 OpenMMLab项目中的重要概念——配置文件

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214844737-850603366.png)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214856690-646029280.png)

## 9.6 MMDetection代码库结构

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214906659-770122354.png)


## 9.7 配置文件的运作方式

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214917714-755977259.png)


## 9.8 两阶段检测器的构成

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214928047-714359842.png)


## 9.9 单阶段检测器的构成

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206214939945-1753949082.png)


## 9.10 RetinaNet模型配置-主干网络

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206223753410-52657539.png)


### 9.10.1 RetinaNet模型配置-颈部

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206223802260-459511305.png)


### 9.10.2 RetinaNet模型配置-bbox head1

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206223815788-1344475004.png)


### 9.10.3 RetinaNet模型配置-bbox head2

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206223941494-1177564853.png)


## 9.11 COCO数据集介绍

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206223954348-50265259.png)


### 9.11.1 COCO数据集格式

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206224008007-1716801155.png)


### 9.11.2 COCO数据集的标注格式

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206224019673-698556491.png)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206224050149-1412799694.png)


### 9.11.3 BBOX标注格式

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206224104006-741052985.png)


### 9.11.4 标注、类别、图像id的对应关系

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206224118046-772553225.png)


## 9.12 在MMDetection中配置COCO数据集

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206224126297-468961756.png)


### 9.12.1 MMDetection中的自定义数据集格式

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206224137095-1188929988.png)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206224146248-454506414.png)


### 9.12.2 数据处理流水线

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206224201156-1579995795.png)

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206224213499-1489206317.png)

## 9.13 MMDetection中的常用训练策略

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206224226359-1051415694.png)


## 9.14 训练自己的检测模型

![](https://img2023.cnblogs.com/blog/1571518/202302/1571518-20230206224234059-967296233.png)

